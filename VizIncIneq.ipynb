{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VizIncIneq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sangttruong/IncomeVis/blob/master/VizIncIneq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr1e3YZuRikN",
        "colab_type": "text"
      },
      "source": [
        "#**Technical Appendix for Visualizing Income Inequality in the United States**\n",
        "\n",
        "[Sang Truong](mailto:sangtruong_2021@depauw.edu) and [Prof. Humberto Barreto](mailto:hbarreto@depauw.edu)\n",
        "\n",
        "Department of Economics and Management, DePauw University, Greencastle, IN 46135\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4_KEipYGDOE",
        "colab_type": "text"
      },
      "source": [
        "## **Section 1. Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4MJifrArn_e",
        "colab_type": "text"
      },
      "source": [
        "This notebook supports Barreto and Truong's 2020 paper, \"Visualizing Income Inequality in the United States.\" It gives a detailed explanation of the transformation of the raw data to the 3D visualization. First, we import `pandas` for data manipulation and `numpy` for scientific computing. We mount Google Drive to Google Colab to read source files directly from Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhyEoROCRflY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "input_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/input/'\n",
        "out_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/output/'\n",
        "\n",
        "# Display 1 decimal\n",
        "pd.options.display.float_format = '{:,.1f}'.format\n",
        "# Show all columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgm4rBtF7Jru",
        "colab_type": "text"
      },
      "source": [
        "Data for analysis are from [IPUMS-CPS](https://cps.ipums.org/cps/). We download all available years in one data extract from CPS. Documentation on downloading and variables can be found on the IPUMS website [1]. For IPUMS-CPS, all selected samples are ASEC. Although IPUMS CPS has data back to 1962, geographic location (STATEFIP) is only available since 1977: \"STATEFIP is comparable for 1963-1967 and 1977 onward, years in which each state and the District of Columbia were separately identified. In the remaining years, two or more states share the same code, and these groupings change over time. In 1962, 8 states cannot be separately identified. In 1968-1972, 32 states cannot be separately identified, and in 1973-1976, 38 states cannot be separately identified. In these years, up to 5 states share the same code.\"\n",
        "\n",
        "We download ASEC samples from 1977 to 2019 as csv in a zip file, extract (~600MB), rename as \"ipums-cps.csv\" and place it in an accessible Google drive location.\n",
        "\n",
        "For updating, note that ASEC comes out in IPUMS in October.\n",
        "\n",
        "Years are confusing. We have ASEC CPS from 1977 to 2019. The CPI99 and HHINCOME variables in a sample year is for the previous year. So, the 1977 sample has Consumr Price Index and Household Income for 1976. Thus, we compute RHHINCOME from sample year 1977 data that is actually real household income for 1976.\n",
        "\n",
        "Variables list: CPI99 STATEFIP HHINCOME PERNUM RELATE AGE SEX RACE HISPAN EDUC (only the first three are actually needed -- we thought we could do visualizations of subgroups, but sample sizes are too small).\n",
        "\n",
        "\"The Census Bureau fielded the CPS 2014 ASEC sample using an experimental redesign. All respondents received new health insurance questions, but 3/8ths of the total sample was randomly selected to receive the redesigned income questions. The larger portion of the sample (5/8) was given the existing questions on income. The redesign attempted to address income under-reporting, in particular, retirement, pensions, annuities, and government cash-transfer programs. More accurate income reporting in turn allows for better measurement of poverty statistics.\" HFLAG = 0 indicates 5/8 sample, and HFLAG = 1 indicate 3/8 sample in 2014. For the 2014 to be compatible with other data year, we select HFLAG = 0. [3]\n",
        "\n",
        "We also create labels for states and establish starting color scheme for the visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv-Ye4Er7Sbb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "outputId": "6ac6c244-133e-4b92-d234-94c0bebd8c13"
      },
      "source": [
        "# Import raw data\n",
        "raw = pd.read_csv(input_path + 'ipums-cps.csv')\n",
        "raw = raw[raw.YEAR >= 1977] # This line should be here regardless if you have the data before 1977 or not. It won't hurt if you don't have the data.\n",
        "\n",
        "# Import rpp and merge with raw \n",
        "rpp = pd.read_csv('gdrive/My Drive/Colab Notebooks/USIncomeVis/input/rpp.csv')\n",
        "raw = pd.merge(raw, rpp, how='outer', on = ['YEAR', 'STATEFIP'])\n",
        "\n",
        "# Select data with HFLAG != 1 and then drop HFLAG\n",
        "raw = raw[raw.HFLAG !=1]\n",
        "raw = raw.drop(columns = ['HFLAG'])\n",
        "print(raw.describe())\n",
        "\n",
        "# States labels and color scheme\n",
        "statefip = list(set(raw.STATEFIP))\n",
        "state_name = ['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',\n",
        "              'Colorado', 'Connecticut', 'Delaware', 'District of Columbia',\n",
        "              'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana',\n",
        "              'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n",
        "              'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri',\n",
        "              'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey',\n",
        "              'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio',\n",
        "              'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina',\n",
        "              'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia',\n",
        "              'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']\n",
        "\n",
        "colors = ['#FF0000', '#FF0A00', '#FF1400', '#FF1E00', '#FF2800', '#FF3300', '#FF3D00',\n",
        "          '#FF4700', '#FF5100', '#FF5B00', '#FF6600', '#FF7000', '#FF7A00', '#FF8400',\n",
        "          '#FF8E00', '#FF9900', '#FFA300', '#FFAD00', '#FFB700', '#FFC100', '#FFCC00',\n",
        "          '#FFD600', '#FFE000', '#FFEA00', '#FFF400', '#FFFF00', '#F4FF00', '#EAFF00',\n",
        "          '#E0FF00', '#D6FF00', '#CCFF00', '#C1FF00', '#B7FF00', '#ADFF00', '#A3FF00',\n",
        "          '#99FF00', '#8EFF00', '#84FF00', '#7AFF00', '#70FF00', '#66FF00', '#5BFF00',\n",
        "          '#51FF00', '#47FF00', '#3DFF00', '#32FF00', '#28FF00', '#1EFF00', '#14FF00',\n",
        "          '#0AFF00', '#00FF00']\n",
        "colors = pd.DataFrame(colors, columns = ['Color'], index = statefip)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             YEAR      SERIAL       MONTH                CPSID    ASECFLAG  \\\n",
            "count 7,477,891.0 7,477,891.0 7,477,891.0          5,526,971.0 7,477,891.0   \n",
            "mean      1,999.0    42,834.1         3.0 14,609,170,480,628.5         1.0   \n",
            "std          12.4    25,731.5         0.0  8,898,738,244,648.0         0.0   \n",
            "min       1,977.0         1.0         3.0                  0.0         1.0   \n",
            "25%       1,988.0    20,960.0         3.0                  0.0         1.0   \n",
            "50%       2,001.0    41,657.0         3.0 19,970,300,219,500.0         1.0   \n",
            "75%       2,010.0    62,644.0         3.0 20,080,101,688,900.0         1.0   \n",
            "max       2,019.0    99,986.0         3.0 20,190,307,098,800.0         1.0   \n",
            "\n",
            "          ASECWTH       CPI99    STATEFIP    HHINCOME      PERNUM  \\\n",
            "count 7,477,891.0 7,477,891.0 7,477,891.0 7,477,891.0 7,477,891.0   \n",
            "mean      1,529.2         1.2        28.0    58,051.0         2.3   \n",
            "std         911.3         0.6        15.7    67,951.9         1.4   \n",
            "min           0.0         0.7         1.0   -37,040.0         1.0   \n",
            "25%         870.1         0.8        13.0    20,022.0         1.0   \n",
            "50%       1,489.8         1.0        29.0    40,000.0         2.0   \n",
            "75%       1,967.1         1.5        41.0    74,000.0         3.0   \n",
            "max      17,957.5         2.9        56.0 3,299,997.0        26.0   \n",
            "\n",
            "                    CPSIDP      ASECWT      RELATE         AGE         SEX  \\\n",
            "count          5,526,971.0 7,477,891.0 7,477,891.0 7,477,891.0 7,477,891.0   \n",
            "mean  14,609,170,480,630.3     1,551.7       270.1        34.3         1.5   \n",
            "std    8,898,738,244,648.2       944.5       252.4        22.1         0.5   \n",
            "min                    0.0         0.0       101.0         0.0         1.0   \n",
            "25%                    0.0       870.4       101.0        15.0         1.0   \n",
            "50%   19,970,300,219,501.0     1,495.7       201.0        33.0         2.0   \n",
            "75%   20,080,101,688,901.5     2,001.2       301.0        51.0         2.0   \n",
            "max   20,190,307,098,801.0    19,982.8     1,260.0        99.0         2.0   \n",
            "\n",
            "             RACE      HISPAN        EDUC         RPP  \n",
            "count 7,477,891.0 7,477,891.0 7,477,891.0 2,096,763.0  \n",
            "mean        146.0        34.6        58.9        99.6  \n",
            "std         138.1       112.9        39.8         9.3  \n",
            "min         100.0         0.0         1.0        85.2  \n",
            "25%         100.0         0.0        20.0        91.4  \n",
            "50%         100.0         0.0        72.0        97.9  \n",
            "75%         100.0         0.0        81.0       106.8  \n",
            "max         830.0       902.0       125.0       119.2  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX4F3Ltva5iT",
        "colab_type": "text"
      },
      "source": [
        "## **Section 2. adjustIncome() Function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FulCvouusuY",
        "colab_type": "text"
      },
      "source": [
        "In this analysis, we employ 3 deflators to make better comparisons: 1) Consumer Price Index (CPI) to create real dollar values of household income over time (RHHINCOME), 2) Household size (HHSIZE) to adjust for the number of people in a household (ERHHINCOME), and 3) State price\n",
        "\n",
        "1) Because of inflation, to compare incomes over time, we need to convert nominal dollar values of HHINCOME to real values. We adjust incomes so they are all in 2018 dollars. \n",
        "\n",
        "2) We want to compare household incomes, but a household with two people with a total income of $100,000 is better off than a six-person household with the same income. We could just divide by the number of people in the household, but that ignores the fact that some expenses don't scale up linearly (a two-bedroom apartment is not double the rent of a one-bedroom and two people could share a car, for example). An *equivalence scale* adjusts household size. There are several options available. We tried the [*OECD equivalence scale*](http://www.oecd.org/els/soc/OECD-Note-EquivalenceScales.pdf) and the *square root equivalence scale*, as explained below.\n",
        "\n",
        "3) \n",
        "\n",
        "Income adjustment can be done via `adjustIncome()` function. Below is its syntax and default parameters.\n",
        "\n",
        "```\n",
        "DataFrame adjustIncome(input_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/input/',\n",
        "                       output_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/output/',\n",
        "                       ipumcps_filename = 'ipums-cps.csv',\n",
        "                       rpp_filename = 'rpp.csv')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOrQucBJqVJo",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title **adjustIncome() Implementation for Developer**\n",
        "from contextlib import contextmanager\n",
        "import os, sys\n",
        "@contextmanager\n",
        "def suppress_stdout():\n",
        "    with open(os.devnull, \"w\") as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        sys.stdout = devnull\n",
        "        try: yield\n",
        "        finally: sys.stdout = old_stdout\n",
        "        \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "from google.colab import drive\n",
        "with suppress_stdout(): drive.mount('/content/gdrive')\n",
        "\n",
        "# Display 1 decimal\n",
        "pd.options.display.float_format = '{:,.1f}'.format\n",
        "# Show all columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "def adjustIncome(input_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/input/',\n",
        "                 output_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/input/',\n",
        "                 ipumcps_filename = 'ipums-cps.csv',\n",
        "                 rpp_filename = 'rpp.csv'):\n",
        "  # Import raw data\n",
        "  raw = pd.read_csv(input_path + ipumcps_filename)\n",
        "  raw = raw[raw.YEAR >= 1977] # This line should be here regardless if you have the data before 1977 or not. It won't hurt if you don't have the data.\n",
        "\n",
        "  # Import rpp and merge with raw \n",
        "  rpp = pd.read_csv(input_path + rpp_filename)\n",
        "  raw = pd.merge(raw, rpp, how='outer', on = ['YEAR', 'STATEFIP'])\n",
        "\n",
        "  # Select data with HFLAG != 1 and then drop HFLAG\n",
        "  raw = raw[raw.HFLAG !=1]\n",
        "  raw = raw.drop(columns = ['HFLAG'])\n",
        "\n",
        "  # Generate Real HHINCOME in 2018 dollars\n",
        "  raw['RHHINCOME'] = raw['HHINCOME']*raw['CPI99']*1.507\n",
        "\n",
        "  # Generate size\n",
        "  raw['ISIZE'] = np.nan\n",
        "  raw.loc[raw['PERNUM'] == 1, 'ISIZE'] = 1\n",
        "  raw.loc[(raw['PERNUM'] != 1) & (raw['AGE'] > 16), 'ISIZE'] = 1\n",
        "  raw.loc[(raw['PERNUM'] != 1) & (raw['AGE'] <= 16), 'ISIZE'] = 1\n",
        "\n",
        "  # Generate household ID\n",
        "  raw['HHID'] = np.nan\n",
        "  length = sum(raw.loc[raw['PERNUM'] == 1, 'PERNUM'])\n",
        "  raw.loc[raw['PERNUM'] == 1, 'HHID'] = np.arange(length)\n",
        "  raw = raw.fillna(method='pad')\n",
        "\n",
        "  # Generate effective size and merge with raw\n",
        "  hhsize = raw.groupby(['HHID'])['ISIZE'].agg('sum').reset_index()\n",
        "  raw = pd.merge(raw, hhsize, on = ['HHID'])\n",
        "  raw = raw.rename(columns={'ISIZE_x': 'ISIZE', 'ISIZE_y': 'HHSIZE'})\n",
        "\n",
        "  # Eliminate observations that has PERNUM != 1\n",
        "  raw = raw[raw['PERNUM'] == 1]\n",
        "\n",
        "  # Squared root of HHSIZE\n",
        "  raw['HHSIZE'] = (raw['HHSIZE'])**(1/2)\n",
        "\n",
        "  # Generate Real HHINCOME in 2018 dollars\n",
        "  raw['ERHHINCOME'] = raw['RHHINCOME']/raw['HHSIZE']\n",
        "\n",
        "  # Adjusting for state price using RPP\n",
        "  raw['RPPRHHINCOME'] = raw['RHHINCOME']/(raw['RPP']/100)\n",
        "  raw['RPPERHHINCOME'] = raw['ERHHINCOME']/(raw['RPP']/100)\n",
        "\n",
        "  # Generate cumulative weight and percentage\n",
        "  raw['CUMWTH'] = np.nan\n",
        "  raw['PERCENTH'] = np.nan\n",
        "\n",
        "  raw.to_csv(output_path + 'processed' + ipumcps_filename, index=False)\n",
        "  return raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5yt7oVptqUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processedRaw = adjustIncome()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGadNMJHRBRe",
        "colab_type": "text"
      },
      "source": [
        "### **2.1. Consumer Price Index**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0OX-vssEHAo",
        "colab_type": "text"
      },
      "source": [
        "CPI reported by IPUMS-CPS is based on 1999 dollars. We convert CPI99 to 2018 dollars by multiplying by 1.507. Remember, CPI in a given sample year is actually the CPI for the previous year.\n",
        "\n",
        "[This web page](https://cps.ipums.org/cps/cpi99.shtml) gives more explanation.\n",
        "[HHINCOME](https://cps.ipums.org/cps-action/variables/HHINCOME#description_section) reports the total money income during the **previous calendar** year of all adult household members. \"The amount should equal the sum of all household members' individual incomes as recorded in the IPUMS-CPS variable [INCTOT](https://cps.ipums.org/cps-action/variables/INCTOT#description_section). The persons included were those present in the household at the time of the survey. People who lived in the household during the previous year but were not still living there at the time of the survey are not included; household members who lived elsewhere during the previous year but had joined the household at the time of the survey are included.\"\n",
        "\n",
        "HHINCOME includes sources of income like wages, salaries, and business income. It can be negative. Some of the component income sources (like [INCWAGE](https://cps.ipums.org/cps-action/variables/INCWAGE#description_section) have \"disclosure avoidance measures\" for individuals with high incomes. Our income range is from the 5th to the 95th percentile so we avoid negative values and problems with correctly measuring extremely high incomes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThLIbcem1-lI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "1ec9c1e2-590c-44bf-f846-48e488e2a156"
      },
      "source": [
        "# Generate Real HHINCOME in 2018 dollars\n",
        "raw['RHHINCOME'] = raw['HHINCOME']*raw['CPI99']*1.507\n",
        "print(raw['RHHINCOME'].describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count   7,477,891.0\n",
            "mean       85,836.7\n",
            "std        81,624.5\n",
            "min       -78,600.3\n",
            "25%        36,943.6\n",
            "50%        68,268.7\n",
            "75%       110,405.1\n",
            "max     3,451,328.3\n",
            "Name: RHHINCOME, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JbQM5NRSN5v",
        "colab_type": "text"
      },
      "source": [
        "### **2.2. Household Size SQRT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UkJ3HyuOSf0H"
      },
      "source": [
        "Since our unit of analysis is household, we initialize a new dataframe (hhsize, or household size) and a new variable (ISIZE, or individual size) to account for the different in household size. For household $j$ in the sample with $n$ members,\n",
        "\n",
        "$$ HHSIZE_j = \\sum_{i = 1}^{n} ISIZE_{n} $$\n",
        "\n",
        "For the OECD equivalence scale, each member of the household contributes to household size depending on age, like this:\n",
        "* Household head (PERNUM = 1): ISIZE = 1 unit.\n",
        "* Adult (PERNUM $\\neq$ 1 and AGE > 16): ISIZE = 0.7 unit.\n",
        "* Child (PERNUM $\\neq$ 1 and AGE $\\leq$ 16): ISIZE = 0.5 unit.\n",
        "\n",
        "For the square root equivalence scale, everyone counts the same. In other words:\n",
        "* Household head (PERNUM = 1): ISIZE = 1 unit.\n",
        "* Adult (PERNUM $\\neq$ 1 and AGE > 16): ISIZE = 1 unit.\n",
        "* Child (PERNUM $\\neq$ 1 and AGE $\\leq$ 16): ISIZE = 1 unit.\n",
        "\n",
        "But the equivalized household size is the square root of the sum of the members in the household. Johnson, et al. 2005, p. 13 explain the logic of the square root scale:\n",
        "\n",
        "\"This particular scale is\n",
        "given by the square root of family size and indicates that the\n",
        "resources for a two-person family must be 41 percent more [sqrt(2) is roughly 1.41]\n",
        "than that of a single-person family for the two families to have\n",
        "an equivalent standard of living. In general, the constant\n",
        "elasticity scales are given by \n",
        "$$ family size^e $$\n",
        "in which *e* is the\n",
        "scale elasticity. Notice that if the elasticity equals one, then\n",
        "the scale equals family size; there are no assumed economies\n",
        "of scale in living arrangements and the equivalent resources\n",
        "are simply the per capita resources. Alternatively, if the\n",
        "elasticity equals zero, then there is no adjustment for family\n",
        "size, there are complete economies of scale in living and the\n",
        "marginal cost of another person is zero. Our chosen elasticity\n",
        "of 0.5 lies halfway between these two implausible extremes\n",
        "and results in “equivalent” consumer unit resources.\"\n",
        "\n",
        "We found little difference between the two scales in this analysis. We think household income per equivalized person is better than just household income, but the specific equivalence scale is applied does not matter. [4]\n",
        "\n",
        "To calculate household size computationally efficiently (i.e., to compute household size without having to iterate through nearly 8 million lines of data), we create a new variable called HHID (household ID) by assigning a unique number to each household. With HHID, we can group each household easily.\n",
        "\n",
        "We then compute the household size for each household ID by taking the sum of all individual sizes in the household. After that, we merge HHSIZE with the raw dataset and drop PERNUM since we no longer need it. \n",
        "\n",
        "Now we can adjust RHHINCOME by HHSIZE to get *equivalized real household income*, ERHHINCOME. \n",
        "\n",
        "This is real household income per equivalent person."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8uQTioFWSf0I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "b1976527-5b4b-4ed1-97a0-121e1ec8e660"
      },
      "source": [
        "# Generate size\n",
        "raw['ISIZE'] = np.nan\n",
        "raw.loc[raw['PERNUM'] == 1, 'ISIZE'] = 1\n",
        "raw.loc[(raw['PERNUM'] != 1) & (raw['AGE'] > 16), 'ISIZE'] = 1\n",
        "raw.loc[(raw['PERNUM'] != 1) & (raw['AGE'] <= 16), 'ISIZE'] = 1\n",
        "\n",
        "# Generate household ID\n",
        "raw['HHID'] = np.nan\n",
        "length = sum(raw.loc[raw['PERNUM'] == 1, 'PERNUM'])\n",
        "raw.loc[raw['PERNUM'] == 1, 'HHID'] = np.arange(length)\n",
        "raw = raw.fillna(method='pad')\n",
        "\n",
        "# Generate effective size and merge with raw\n",
        "hhsize = raw.groupby(['HHID'])['ISIZE'].agg('sum').reset_index()\n",
        "raw = pd.merge(raw, hhsize, on = ['HHID'])\n",
        "raw = raw.rename(columns={'ISIZE_x': 'ISIZE', 'ISIZE_y': 'HHSIZE'})\n",
        "\n",
        "# Eliminate observations that has PERNUM != 1\n",
        "raw = raw[raw['PERNUM'] == 1]\n",
        "\n",
        "# Squared root of HHSIZE\n",
        "raw['HHSIZE'] = (raw['HHSIZE'])**(1/2)\n",
        "\n",
        "# Generate Real HHINCOME in 2018 dollars\n",
        "raw['ERHHINCOME'] = raw['RHHINCOME']/raw['HHSIZE']\n",
        "\n",
        "print(raw[['ISIZE', 'PERNUM', 'AGE', 'HHID', 'ERHHINCOME', 'HHSIZE']].head(5))\n",
        "print(raw[['ISIZE', 'PERNUM', 'AGE', 'HHID', 'ERHHINCOME', 'HHSIZE']].describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    ISIZE  PERNUM  AGE  HHID  ERHHINCOME  HHSIZE\n",
            "0     1.0       1   48   0.0    35,881.2     1.4\n",
            "2     1.0       1   47   1.0    87,438.2     2.2\n",
            "7     1.0       1   63   2.0    72,742.1     1.4\n",
            "9     1.0       1   29   3.0    27,472.5     1.4\n",
            "11    1.0       1   29   4.0    29,616.7     2.0\n",
            "            ISIZE      PERNUM         AGE        HHID  ERHHINCOME      HHSIZE\n",
            "count 2,763,219.0 2,763,219.0 2,763,219.0 2,763,219.0 2,763,219.0 2,763,219.0\n",
            "mean          1.0         1.0        48.4 1,381,609.0    47,773.9         1.6\n",
            "std           0.0         0.0        17.0   797,672.8    47,075.3         0.4\n",
            "min           1.0         1.0         0.0         0.0   -52,725.6         1.0\n",
            "25%           1.0         1.0        35.0   690,804.5    20,260.0         1.4\n",
            "50%           1.0         1.0        46.0 1,381,609.0    37,183.8         1.4\n",
            "75%           1.0         1.0        61.0 2,072,413.5    61,021.6         2.0\n",
            "max           1.0         1.0        99.0 2,763,218.0 2,122,072.5         5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b_dwJ6MpEMl",
        "colab_type": "text"
      },
      "source": [
        "### **2.3. State price**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLdiRKgnbddt",
        "colab_type": "text"
      },
      "source": [
        "Prices in different states vary widely. According to the BEA, in 2018, for example, Alabama’s state price level for all items is 86.4. California was a third higher at 115.4. Thus, adjusting for price differences across states matters. We use the [BEA's Regional Price](https://www.bea.gov/data/prices-inflation/regional-price-parities-state-and-metro-area) Parities data from 2008 (the earliest year) to 2018 to adjust RHHINCOME and ERHHINCOME. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMkxopURpI2g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "4400e87d-84a7-4365-b591-42cf657bf04d"
      },
      "source": [
        "# Adjusting for state price using RPP\n",
        "raw['RPPRHHINCOME'] = raw['RHHINCOME']/(raw['RPP']/100)\n",
        "raw['RPPERHHINCOME'] = raw['ERHHINCOME']/(raw['RPP']/100)\n",
        "\n",
        "# Generate cumulative weight and percentage\n",
        "raw['CUMWTH'] = np.nan\n",
        "raw['PERCENTH'] = np.nan\n",
        "\n",
        "print(raw[['ERHHINCOME', 'RHHINCOME', 'RPPRHHINCOME', 'RPPERHHINCOME', 'RPP', 'CUMWTH', 'PERCENTH']].head(5))\n",
        "raw.to_csv(input_path + 'processedRaw.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    ERHHINCOME  RHHINCOME  RPPRHHINCOME  RPPERHHINCOME  RPP  CUMWTH  PERCENTH\n",
            "0     35,881.2   50,743.7           nan            nan  nan     nan       nan\n",
            "2     87,438.2  195,517.7           nan            nan  nan     nan       nan\n",
            "7     72,742.1  102,872.9           nan            nan  nan     nan       nan\n",
            "9     27,472.5   38,852.0           nan            nan  nan     nan       nan\n",
            "11    29,616.7   59,233.3           nan            nan  nan     nan       nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTF32ok9o0JN",
        "colab_type": "text"
      },
      "source": [
        "## **Section 3. getIncomeVis() Function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMvMCeKV_hyq",
        "colab_type": "text"
      },
      "source": [
        "After the 1977 demonstration, we use `getIncomeVis()` function to process data for samples from `year_start` to `year_end`, which yields HHINCOME from `year_start-1` to `year_end-1`. Following is syntax of `getIncomeVis()` function with its default parameters:\n",
        "\n",
        "```\n",
        "(DataFrame) getIncomeVis(incomeType = 'RHHINCOME',\n",
        "                         k = 'decile',\n",
        "                         year_start = 1977,\n",
        "                         year_end = 2019, \n",
        "                         input_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/input/' + 'processedRaw.csv', \n",
        "                         output_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/output/',\n",
        "                         toState = True,\n",
        "                         provide_colorFrame = False,\n",
        "                         colorFrame = pd.DataFrame(),\n",
        "                         returnColor = False)\n",
        "```\n",
        "\n",
        "*   `incomeType(String)`: either `'HHINCOME'`, `'RHHINCOME'`, `'ERHHINCOME'`, `'RPPRHHINCOME'`, or `'RPPERHHINCOME'`.\n",
        "*   `k(String)`: either `'decile'` or `'percentile'`. You must have `decile` and `percentile` folders in output directory to store output.\n",
        "*   `year_start(int)`: starting year.\n",
        "*   `year_end(int)`: ending year (unlike conventional index in Python, this year will also be included).\n",
        "*   `input_path(String)`: file path of `processedRaw.csv` file.\n",
        "*   `output_path(String)`: directory to output result.\n",
        "*   `toState(bool)`: if `True`, output State graphs for the specified range (also output csv file for both Year and State graphs).\n",
        "*   `provide_colorFrame(bool)`: where or not to use a precreated color frame.\n",
        "*   `colorFrame(Pandas DataFrame)`: will be ignore if `provide_colorFrame = False`.\n",
        "*   `returnColor(bool)`: if `True`, will only process year_start and return color frame of that year. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGxmKvRyogGz",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title **getIncomeVis() Implementation for Developer**\n",
        "from contextlib import contextmanager\n",
        "import os, sys\n",
        "@contextmanager\n",
        "def suppress_stdout():\n",
        "    with open(os.devnull, \"w\") as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        sys.stdout = devnull\n",
        "        try: yield\n",
        "        finally: sys.stdout = old_stdout\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "from google.colab import drive\n",
        "with suppress_stdout(): drive.mount('/content/gdrive')\n",
        "def getIncomeVis(incomeType = 'RHHINCOME',\n",
        "                 k = 'decile', \n",
        "                 year_start = 1977,\n",
        "                 year_end = 2019, \n",
        "                 input_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/input/' + 'processedRaw.csv',\n",
        "                 output_path = 'gdrive/My Drive/Colab Notebooks/USIncomeVis/output/',  \n",
        "                 toState = False,\n",
        "                 provide_colorFrame = False, \n",
        "                 colorFrame = pd.DataFrame(), \n",
        "                 returnColor = False\n",
        "                 ):\n",
        "  \n",
        "  # Display 1 decimal\n",
        "  pd.options.display.float_format = '{:,.1f}'.format\n",
        "  \n",
        "  # Show all columns\n",
        "  pd.set_option('display.max_columns', None)\n",
        "  pd.set_option('display.max_rows', None)\n",
        "\n",
        "  # Get preprocessed raw file\n",
        "  raw = pd.read_csv(input_path)\n",
        "\n",
        "  # Generate statefip\n",
        "  statefip = list(set(raw.STATEFIP))\n",
        "\n",
        "  for year in range(year_start, year_end+1):\n",
        "    # Generate year_df dataframe\n",
        "    year_df = raw[raw.YEAR == year]\n",
        "\n",
        "    # Labels \n",
        "    label_list = ['']\n",
        "\n",
        "    state_name = ['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',\n",
        "                  'Colorado', 'Connecticut', 'Delaware', 'District of Columbia',\n",
        "                  'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana',\n",
        "                  'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n",
        "                  'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri',\n",
        "                  'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey',\n",
        "                  'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio',\n",
        "                  'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina',\n",
        "                  'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia',\n",
        "                  'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']\n",
        "\n",
        "    colors = ['#FF0000', '#FF0A00', '#FF1400', '#FF1E00', '#FF2800', '#FF3300', '#FF3D00',\n",
        "              '#FF4700', '#FF5100', '#FF5B00', '#FF6600', '#FF7000', '#FF7A00', '#FF8400',\n",
        "              '#FF8E00', '#FF9900', '#FFA300', '#FFAD00', '#FFB700', '#FFC100', '#FFCC00',\n",
        "              '#FFD600', '#FFE000', '#FFEA00', '#FFF400', '#FFFF00', '#F4FF00', '#EAFF00',\n",
        "              '#E0FF00', '#D6FF00', '#CCFF00', '#C1FF00', '#B7FF00', '#ADFF00', '#A3FF00',\n",
        "              '#99FF00', '#8EFF00', '#84FF00', '#7AFF00', '#70FF00', '#66FF00', '#5BFF00',\n",
        "              '#51FF00', '#47FF00', '#3DFF00', '#32FF00', '#28FF00', '#1EFF00', '#14FF00',\n",
        "              '#0AFF00', '#00FF00']\n",
        "\n",
        "    # Create state_Name and state_label dataframes\n",
        "    notLabelList = list(set(state_name) - set(label_list))\n",
        "    notLabelDict = dict.fromkeys(notLabelList , '')\n",
        "    state_name = pd.DataFrame(data = state_name, index = statefip, columns = ['State'])\n",
        "    state_label = state_name.replace(to_replace = notLabelDict)\n",
        "    state_label = state_label.rename(columns={'State': 'Label'})\n",
        "\n",
        "    # Create decile and percentile arrays\n",
        "    decile = np.arange(0.05, 1.05, 0.1) # 10\n",
        "    decile = np.insert(arr = decile, obj = 5, values = 0.5) # 11\n",
        "    \n",
        "    percentile = np.arange(0.02, 1, 0.01) # 98\n",
        "\n",
        "\t  # Create decile and percentile name\n",
        "    decileName = ['5p', '15p', '25p', '35p', '45p', '50p', '55p', '65p', '75p', '85p', '95p']\n",
        "    percentileName = ['2p', '3p', '4p', '5p', '6p', '7p', '8p', '9p', '10p', '11p',\n",
        "                  \t\t'12p', '13p', '14p', '15p', '16p', '17p', '18p', '19p', '20p',\n",
        "                \t\t  '21p', '22p', '23p', '24p', '25p', '26p', '27p', '28p', '29p',\n",
        "                  \t\t'30p', '31p', '32p', '33p', '34p', '35p', '36p', '37p', '38p',\n",
        "                  \t\t'39p', '40p', '41p', '42p', '43p', '44p', '45p', '46p', '47p',\n",
        "                  \t\t'48p', '49p', '50p', '51p', '52p', '53p', '54p', '55p', '56p',\n",
        "                  \t\t'57p', '58p', '59p', '60p', '61p', '62p', '63p', '64p', '65p',\n",
        "                  \t\t'66p', '67p', '68p', '69p', '70p', '71p', '72p', '73p', '74p',\n",
        "                  \t\t'75p', '76p', '77p', '78p', '79p', '80p', '81p', '82p', '83p',\n",
        "                  \t\t'84p', '85p', '86p', '87p', '88p', '89p', '90p', '91p', '92p',\n",
        "                  \t\t'93p', '94p', '95p', '96p', '97p', '98p', '99p']\n",
        "                  \t\t\n",
        "    if (k == 'decile'):\n",
        "      k_ile = decile\n",
        "      kName = decileName\n",
        "    elif (k == 'percentile'):\n",
        "      k_ile = percentile\n",
        "      kName = percentileName\n",
        "    else: print(\"Invalid k\")\n",
        "\n",
        "    # Generate result grid, decile-column\n",
        "    result = pd.DataFrame(data = None, index = kName, columns = statefip)\n",
        "\n",
        "    # Iterate through each state\n",
        "    c = 0\n",
        "    for i in statefip:\n",
        "      # Generate state dataframe\n",
        "      state_df = year_df[year_df.STATEFIP == i]\n",
        "      state_df = state_df.reset_index(drop = True)\n",
        "\n",
        "      # Sort state dataframe by RHHINCOME\n",
        "      state_df = state_df.sort_values(incomeType)\n",
        "    \n",
        "      # Calculate cumulated weight and Percentage\n",
        "      state_df.CUMWTH = state_df.ASECWTH.cumsum()\n",
        "      state_df.PERCENTH = state_df.CUMWTH/(state_df.ASECWTH.sum())\n",
        "\n",
        "      # Calculate decile\n",
        "      r = 0\n",
        "      for _k_ile in k_ile:\n",
        "        result.iloc[r,c] = state_df.loc[state_df['PERCENTH'] <= _k_ile, incomeType].max()\n",
        "        r = r + 1\n",
        "      c = c + 1\n",
        "\n",
        "    # Transpose result table: column-decile\n",
        "    result = result.T\n",
        "\n",
        "    # Sort the result by median\n",
        "    result = result.sort_values(by = ['50p'], ascending = True)\n",
        "\n",
        "    colors = ['#FF0000', '#FF0A00', '#FF1400', '#FF1E00', '#FF2800', '#FF3300', '#FF3D00',\n",
        "              '#FF4700', '#FF5100', '#FF5B00', '#FF6600', '#FF7000', '#FF7A00', '#FF8400',\n",
        "              '#FF8E00', '#FF9900', '#FFA300', '#FFAD00', '#FFB700', '#FFC100', '#FFCC00',\n",
        "              '#FFD600', '#FFE000', '#FFEA00', '#FFF400', '#FFFF00', '#F4FF00', '#EAFF00',\n",
        "              '#E0FF00', '#D6FF00', '#CCFF00', '#C1FF00', '#B7FF00', '#ADFF00', '#A3FF00',\n",
        "              '#99FF00', '#8EFF00', '#84FF00', '#7AFF00', '#70FF00', '#66FF00', '#5BFF00',\n",
        "              '#51FF00', '#47FF00', '#3DFF00', '#32FF00', '#28FF00', '#1EFF00', '#14FF00',\n",
        "              '#0AFF00', '#00FF00']\n",
        "    colors = pd.DataFrame(colors, columns = ['Color'], index = statefip)\n",
        "    \n",
        "    if(not provide_colorFrame): colorFrame = pd.DataFrame(data = list(colors.Color), index = result.index, columns=['Color'])\n",
        "    result = pd.concat([state_name, result, state_label, colorFrame], axis = 1)\n",
        "\n",
        "    if (returnColor): return colorFrame\n",
        "\n",
        "    # Compute state population and normalized state population\n",
        "    pop = pd.DataFrame(index = statefip)\n",
        "    pop['POP'] = year_df.groupby(['STATEFIP'])['ASECWT'].agg('sum')\n",
        "    pop['NORMPOP'] = round(pop['POP']/(np.percentile(pop['POP'], 10)))\n",
        "\n",
        "    # Replicate each state's dataline with its respective replication number\n",
        "    for i in statefip:\n",
        "      rep = pop.loc[i,'NORMPOP'] - 1\n",
        "      rep = int(rep)\n",
        "      line = pd.DataFrame(result.loc[i]).T\n",
        "      line.loc[i, 'Label'] = ''\n",
        "      for i in range(0,rep): result = pd.concat([result, line]) \n",
        "\n",
        "    # Sort the result by median\n",
        "    result = result.sort_values(by = ['50p', 'State'], ascending = True)\n",
        "\n",
        "    # Add the middle property\n",
        "    result.reset_index(drop = True, inplace = True)\n",
        "\n",
        "    result['Middle'] = np.nan\n",
        "    counter = 0\n",
        "    for state in result.State.drop_duplicates():\n",
        "      temp = result[result.State == state]\n",
        "      temp_size = len(temp.index)\n",
        "      middle = (temp_size // 2)\n",
        "      counter = counter + middle\n",
        "      result.loc[counter, 'Middle'] = 1\n",
        "      counter = counter - middle + temp_size\n",
        "\n",
        "    # Output csv file if csv_output is True\n",
        "    if (toState): result.to_csv(output_path + k + '/' + 'YEAR' + str(year-1) + '_' + incomeType + '.csv', index = False)\n",
        "    \n",
        "    # Convert dataframe to JSON\n",
        "    result = result.to_json(orient = 'records')\n",
        "    result = json.loads(result, object_pairs_hook = OrderedDict)\n",
        "\n",
        "    # Make JSON format readable\n",
        "    result = json.dumps(result, indent = 4, sort_keys = False)\n",
        "\n",
        "    # Save JSON file -- y-1 adjusts sample year_df to HHINCOME year_df\n",
        "    with open(output_path + k + '/' + 'YEAR' + str(year-1) + '_' + incomeType + '.js', 'w') as outfile:\n",
        "      outfile.write('var data =')\n",
        "      outfile.write(result)\n",
        "\n",
        "  if (toState):\n",
        "    if (k == 'decile'):\n",
        "      column_df = ['5p', '15p', '25p', '35p', '45p', '50p', '55p', '65p', '75p','85p', '95p', 'Label']\n",
        "      new_column = ['Year', '5p', '15p', '25p', '35p', '45p', '50p', '55p', '65p', '75p','85p', '95p', 'Label']\n",
        "    elif (k == 'percentile'):\n",
        "      column_df = ['2p', '3p', '4p', '5p', '6p', '7p', '8p', '9p', '10p', '11p',\n",
        "                   '12p', '13p', '14p', '15p', '16p', '17p', '18p', '19p', '20p',\n",
        "                   '21p', '22p', '23p', '24p', '25p', '26p', '27p', '28p', '29p',\n",
        "                   '30p', '31p', '32p', '33p', '34p', '35p', '36p', '37p', '38p',\n",
        "                   '39p', '40p', '41p', '42p', '43p', '44p', '45p', '46p', '47p',\n",
        "                   '48p', '49p', '50p', '51p', '52p', '53p', '54p', '55p', '56p',\n",
        "                   '57p', '58p', '59p', '60p', '61p', '62p', '63p', '64p', '65p',\n",
        "                   '66p', '67p', '68p', '69p', '70p', '71p', '72p', '73p', '74p',\n",
        "                   '75p', '76p', '77p', '78p', '79p', '80p', '81p', '82p', '83p',\n",
        "                   '84p', '85p', '86p', '87p', '88p', '89p', '90p', '91p', '92p',\n",
        "                   '93p', '94p', '95p', '96p', '97p', '98p', '99p', 'Label']\n",
        "      new_column = ['Year', '2p', '3p', '4p', '5p', '6p', '7p', '8p', '9p', '10p', '11p',\n",
        "                    '12p', '13p', '14p', '15p', '16p', '17p', '18p', '19p', '20p',\n",
        "                    '21p', '22p', '23p', '24p', '25p', '26p', '27p', '28p', '29p',\n",
        "                    '30p', '31p', '32p', '33p', '34p', '35p', '36p', '37p', '38p',\n",
        "                    '39p', '40p', '41p', '42p', '43p', '44p', '45p', '46p', '47p',\n",
        "                    '48p', '49p', '50p', '51p', '52p', '53p', '54p', '55p', '56p',\n",
        "                    '57p', '58p', '59p', '60p', '61p', '62p', '63p', '64p', '65p',\n",
        "                    '66p', '67p', '68p', '69p', '70p', '71p', '72p', '73p', '74p',\n",
        "                    '75p', '76p', '77p', '78p', '79p', '80p', '81p', '82p', '83p',\n",
        "                    '84p', '85p', '86p', '87p', '88p', '89p', '90p', '91p', '92p',\n",
        "                    '93p', '94p', '95p', '96p', '97p', '98p', '99p', 'Label']\n",
        "    else: print('Invalid k')\n",
        "\n",
        "    for n in statefip:\n",
        "      #Reformat the columns\n",
        "      data_df = []\n",
        "      index_df = [i for i in range(0, 43)]\n",
        "      for i in range(year_start, year_end+1):\n",
        "        df = pd.read_csv(output_path + k + '/' + 'YEAR' + str(year-1) + '_' + incomeType + '.csv')\n",
        "        df.drop(columns=['Color', 'Middle'], inplace=True)\n",
        "        t = df.loc[n].tolist()\n",
        "        del t[0]\n",
        "        data_df.append(t)\n",
        "\n",
        "      state_df = pd.DataFrame(data_df,columns=column_df,index=index_df)\n",
        "      state_df['Year'] = [i-1 for i in range(year_start, year_end + 1)]\n",
        "      state_df['Label'] = ''\n",
        "\n",
        "      #Convert to new Year formatted .csv files\n",
        "      state_df = state_df.reindex(columns=new_column)\n",
        "      state_df.to_csv(output_path + k + '/' + 'STATE' + str(n) + '_' + incomeType + '.csv', index=False)\n",
        "  \n",
        "      # Convert dataframe to JSON\n",
        "      state_df = state_df.to_json(orient = 'records')\n",
        "      state_df = json.loads(state_df, object_pairs_hook = OrderedDict)\n",
        "\n",
        "      # Make JSON format readable\n",
        "      state_df = json.dumps(state_df, indent = 4, sort_keys = False)\n",
        "\n",
        "      # Save JSON file -- y-1 adjusts sample year to HHINCOME year\n",
        "      with open(output_path + k + '/' + 'STATE' + str(n) + '_' + incomeType + '.js', 'w') as outfile:\n",
        "        outfile.write(\"var data =\")\n",
        "        outfile.write(state_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGgY02Cz_kDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decile\n",
        "colorRHHINCOME1977 = getIncomeVis(incomeType = 'RHHINCOME', returnColor = True)\n",
        "getIncomeVis(incomeType = 'RHHINCOME', provide_colorFrame = True, colorFrame = colorRHHINCOME1977, toState = True)\n",
        "\n",
        "colorERHHINCOME1977 = getIncomeVis(incomeType = 'ERHHINCOME', returnColor = True)\n",
        "getIncomeVis(incomeType = 'ERHHINCOME', provide_colorFrame = True, colorFrame = colorERHHINCOME1977, toState = True)\n",
        "\n",
        "colorRPPRHHINCOME1977 = getIncomeVis(incomeType = 'RPPRHHINCOME', returnColor = True)\n",
        "getIncomeVis(incomeType = 'RPPRHHINCOME', provide_colorFrame = True, colorFrame = colorRPPRHHINCOME1977, toState = True)\n",
        "\n",
        "colorRPPERHHINCOME1977 = getIncomeVis(incomeType = 'RPPERHHINCOME', returnColor = True)\n",
        "getIncomeVis(incomeType = 'RPPERHHINCOME', provide_colorFrame = True, colorFrame = colorRPPERHHINCOME1977, toState = True)\n",
        "\n",
        "colorRHHINCOME1977 = getIncomeVis(incomeType = 'HHINCOME', returnColor = True)\n",
        "getIncomeVis(incomeType = 'HHINCOME', provide_colorFrame = True, colorFrame = colorRHHINCOME1977, toState = True)\n",
        "\n",
        "# Percentile\n",
        "colorRHHINCOME1977 = getIncomeVis(incomeType = 'RHHINCOME', returnColor = True, k = 'percentile')\n",
        "getIncomeVis(incomeType = 'RHHINCOME', provide_colorFrame = True, colorFrame = colorRHHINCOME1977, toState = True, k = 'percentile')\n",
        "\n",
        "colorERHHINCOME1977 = getIncomeVis(incomeType = 'ERHHINCOME', returnColor = True, k = 'percentile')\n",
        "getIncomeVis(incomeType = 'ERHHINCOME', provide_colorFrame = True, colorFrame = colorERHHINCOME1977, toState = True, k = 'percentile')\n",
        "\n",
        "colorRPPRHHINCOME1977 = getIncomeVis(incomeType = 'RPPRHHINCOME', returnColor = True, k = 'percentile')\n",
        "getIncomeVis(incomeType = 'RPPRHHINCOME', provide_colorFrame = True, colorFrame = colorRPPRHHINCOME1977, toState = True, k = 'percentile')\n",
        "\n",
        "colorRPPERHHINCOME1977 = getIncomeVis(incomeType = 'RPPERHHINCOME', returnColor = True, k = 'percentile')\n",
        "getIncomeVis(incomeType = 'RPPERHHINCOME', provide_colorFrame = True, colorFrame = colorRPPERHHINCOME1977, toState = True, k = 'percentile')\n",
        "\n",
        "colorRHHINCOME1977 = getIncomeVis(incomeType = 'HHINCOME', returnColor = True, k = 'percentile')\n",
        "getIncomeVis(incomeType = 'HHINCOME', provide_colorFrame = True, colorFrame = colorRHHINCOME1977, toState = True, k = 'percentile')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kgy9Oi2MkMZS",
        "colab_type": "text"
      },
      "source": [
        "### **3.1. HHINCOME**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyxhSEX3dN7L",
        "colab_type": "text"
      },
      "source": [
        "Our goal is to prepare the data for rendering our final graph in JavaScript format. We segment HHINCOME into percentiles from 5 to 95 in steps of 10 and also the median. The code below can also produce increments of 1 percentile,  but then amCharts (our visualizer) is extremely slow.\n",
        "Since we allow the user to control the states being labeled, we do not label any of them. \n",
        "We also create CUMWTH (cummulated household weight) and PERCENT (percentile for each household) as a way to rank income of each household\n",
        "\n",
        "For the purpose of demostration and explanation, we walk through 1977 decile below to illustrate the mechanism behind the analysis. We do so by breaking the first for loop so that we render 1 graph for 1 state at a time. After that, we generate a result grid, which have the k-iles for each state at a year. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BiPxxVoLetC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "320b46fb-ce10-4b6a-8b35-cd414cd38c40"
      },
      "source": [
        "  # Generate year dataframe -- can enter any year from 1977 to 2019\n",
        "  y = 1977\n",
        "  year = raw[raw.YEAR == y]\n",
        "\n",
        "  # Create decile and percentile arrays\n",
        "  decile = np.arange(0.05, 1.05, 0.1) # 10\n",
        "  decile = np.insert(arr = decile, obj = 5, values = 0.5) # 11\n",
        "  percentile = np.arange(0.02, 1, 0.01) # 98\n",
        "\n",
        "  # Create decile and percentile name\n",
        "  decileName = ['5p', '15p', '25p', '35p', '45p', '50p', '55p', '65p', '75p', '85p', '95p']\n",
        "  percentileName = ['2p', '3p', '4p', '5p', '6p', '7p', '8p', '9p', '10p', '11p',\n",
        "                    '12p', '13p', '14p', '15p', '16p', '17p', '18p', '19p', '20p',\n",
        "                    '21p', '22p', '23p', '24p', '25p', '26p', '27p', '28p', '29p',\n",
        "                    '30p', '31p', '32p', '33p', '34p', '35p', '36p', '37p', '38p',\n",
        "                    '39p', '40p', '41p', '42p', '43p', '44p', '45p', '46p', '47p',\n",
        "                    '48p', '49p', '50p', '51p', '52p', '53p', '54p', '55p', '56p',\n",
        "                    '57p', '58p', '59p', '60p', '61p', '62p', '63p', '64p', '65p',\n",
        "                    '66p', '67p', '68p', '69p', '70p', '71p', '72p', '73p', '74p',\n",
        "                    '75p', '76p', '77p', '78p', '79p', '80p', '81p', '82p', '83p',\n",
        "                    '84p', '85p', '86p', '87p', '88p', '89p', '90p', '91p', '92p',\n",
        "                    '93p', '94p', '95p', '96p', '97p', '98p', '99p']\n",
        "\n",
        "  # Labels \n",
        "  label_list = ['']\n",
        "\n",
        "  # Create state_Name and state_label dataframes\n",
        "  notLabelList = list(set(state_name) - set(label_list))\n",
        "  notLabelDict = dict.fromkeys(notLabelList , '')\n",
        "  state_name = pd.DataFrame(data = state_name, index = statefip, columns = ['State'])\n",
        "  state_label = state_name.replace(to_replace = notLabelDict)\n",
        "  state_label = state_label.rename(columns={'State': 'Label'})\n",
        "\n",
        "  # Choose k: decile (11) or percentile (98)\n",
        "  k = 'decile'\n",
        "  if (k == 'decile'):\n",
        "    k_ile = decile\n",
        "    kName = decileName\n",
        "\n",
        "  if (k == 'percentile'):\n",
        "    k_ile = percentile\n",
        "    kName = percentileName\n",
        "\n",
        "  # Generate result grid, decile-column\n",
        "  result = pd.DataFrame(data = None, index = kName, columns = statefip)\n",
        "  print(result[[1, 2, 4]].head(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       1    2    4\n",
            "5p   NaN  NaN  NaN\n",
            "15p  NaN  NaN  NaN\n",
            "25p  NaN  NaN  NaN\n",
            "35p  NaN  NaN  NaN\n",
            "45p  NaN  NaN  NaN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIk4l7TKHN1b",
        "colab_type": "text"
      },
      "source": [
        "For every state, we sort the adjusted household income ascendingly. We then calculate the household cumulated weight (CUMWTH) and the household percentile (PERCENTH). For household $j$th in the RHHINCOME-sorted dataframe with $k$ households:\n",
        "\n",
        "$$CUMWTH_j = \\sum_{i=0}^{j} ASECWTH_i; PERCENTH_j = \\frac{CUMWTH_j}{\\sum_{i=0}^{k} ASECWTH_i}$$\n",
        "\n",
        "To calculate RHHINCOME at each k_ile $k$ for state $s$, we select a series of RHHINCOME that has percentile under or equal to $k$ (set S):\n",
        "$$S_k = \\{RHHINCOME_s | PERCENTH_s \\leq k \\} $$\n",
        "\n",
        "RHHINCOME at k_ile $k$ for state s is the maximum value of set $S$: \n",
        "\n",
        "$$ RHHINCOME_{k, s} = max(S_k) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9jS5rEQHMjk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "c8529a4a-2ae8-45bc-a33f-51ba488a93f3"
      },
      "source": [
        "  # Iterate through each state\n",
        "  c = 0\n",
        "  for i in statefip:\n",
        "    # Generate state dataframe\n",
        "    state = year[year.STATEFIP == i]\n",
        "    state = state.reset_index(drop = True)\n",
        "\n",
        "    # Sort state dataframe by RHHINCOME\n",
        "    state = state.sort_values('RHHINCOME')\n",
        "    \n",
        "    # Calculate cumulated weight and Percentage\n",
        "    state.CUMWTH = state.ASECWTH.cumsum()\n",
        "    state.PERCENTH = state.CUMWTH/(state.ASECWTH.sum())\n",
        "    \n",
        "    # Calculate decile\n",
        "    r = 0\n",
        "    for _k_ile in k_ile:\n",
        "      result.iloc[r,c] = state.loc[state['PERCENTH'] <= _k_ile, 'RHHINCOME'].max()\n",
        "      r = r + 1\n",
        "    c = c + 1\n",
        "\n",
        "  # Transpose result table: column-decile\n",
        "  result = result.T\n",
        "\n",
        "  # Sort the result by median\n",
        "  result = result.sort_values(by = ['50p'], ascending = True)\n",
        "  colors1977 = pd.DataFrame(data = list(colors.Color), index = result.index, columns=['Color'])\n",
        "  result = pd.concat([state_name, result, state_label, colors1977], axis = 1)\n",
        "  print(result.head(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        State       5p      15p      25p      35p      45p      50p      55p  \\\n",
            "1     Alabama  8,039.6 15,201.0 23,651.0 32,224.5 42,721.8 48,272.7 52,950.0   \n",
            "2      Alaska 12,178.5 34,029.2 48,537.5 66,187.4 81,631.2 88,964.7 98,839.9   \n",
            "4     Arizona  8,229.3 18,183.9 26,916.2 38,318.1 47,434.3 52,839.6 57,362.4   \n",
            "5    Arkansas  8,458.8 13,237.5 19,891.5 26,475.0 34,417.5 37,422.4 41,106.8   \n",
            "6  California 12,160.8 20,015.1 30,887.5 40,330.2 51,855.7 57,450.7 64,051.8   \n",
            "\n",
            "        65p       75p       85p       95p Label    Color  \n",
            "1  63,981.2  77,262.8  94,859.8 131,933.6        #FF5100  \n",
            "2 117,001.7 139,589.3 174,734.8 223,122.3        #00FF00  \n",
            "4  68,609.9  82,513.7  98,853.1 152,897.4        #FFCC00  \n",
            "5  49,742.1  62,414.8  76,243.5 115,519.1        #FF0000  \n",
            "6  77,218.7  92,212.3 114,164.5 166,192.2        #A3FF00  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKHqeTk5m3IK",
        "colab_type": "text"
      },
      "source": [
        "### **3.2. Population**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8315gfdj32a",
        "colab_type": "text"
      },
      "source": [
        "We compute an estimate of the number of households in the population (POP) and then normalize it for each state in a new dataframe (NORMPOP). For state $s$ that has $k$ households in year $y$, we could make each state's population relative to the smallest state, like this:\n",
        "\n",
        "$$POP_s = \\sum_{i=0}^{k} ASECWTH_i; NORMPOP_s = \\frac{POP_s}{min(POP_s)}$$ \n",
        "\n",
        "\n",
        "However, this turns out to produce an ugly graph (with extremely thin slices for the smallest states) and takes a long time to render in a browser. To improve visibility and rendering, we normalize to the 10th percentile which makes about 1/4 of the states have a width of 1 in the graph. California (the most populous state) has a width 34 times the smallest states (in actuality it should be 68 times).\n",
        "\n",
        "We replicate each state dataline with its relative population. Sort the result dataframe by the median to get the 3D visualization to plot the states in ascending order of household income. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6zZyftSj3Pl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "835249f4-2a0e-4e67-c5f1-4b78699a4b2b"
      },
      "source": [
        "  # Compute state population and normalized state population\n",
        "  pop = pd.DataFrame(index = statefip)\n",
        "  pop['POP'] = year.groupby(['STATEFIP'])['ASECWT'].agg('sum')\n",
        "  pop['NORMPOP'] = round(pop['POP']/(pop['POP'].min()))\n",
        "\n",
        "  # Normalization\n",
        "  pop['NORMPOP'] = round(pop['POP']/(np.percentile(pop['POP'],10)))\n",
        "  print(pop.head())\n",
        "  \n",
        "  # Replicate each state's dataline with its respective replication number\n",
        "  for i in statefip:\n",
        "    rep = pop.loc[i,'NORMPOP'] - 1\n",
        "    rep = int(rep)\n",
        "    line = pd.DataFrame(result.loc[i]).T\n",
        "    line.loc[i, 'Label'] = ''\n",
        "    for i in range(0,rep): result = pd.concat([result, line])\n",
        "\n",
        "  # Sort the result by median\n",
        "  result = result.sort_values(by = ['50p', 'State'], ascending = True)\n",
        "  result.reset_index(drop = True, inplace = True)\n",
        "\n",
        "  # Enables the user to add a label to the middle slice in the graph in AmChart\n",
        "  result['Middle'] = np.nan\n",
        "  counter = 0\n",
        "  for state in result.State.drop_duplicates():\n",
        "    temp = result[result.State == state]\n",
        "    temp_size = len(temp.index)\n",
        "    middle = (temp_size // 2)\n",
        "    counter = counter + middle\n",
        "    result.loc[counter, 'Middle'] = 1\n",
        "    counter = counter - middle + temp_size\n",
        "  print(result.head(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          POP  NORMPOP\n",
            "1 1,301,714.0      6.0\n",
            "2   116,622.3      1.0\n",
            "4   789,233.0      3.0\n",
            "5   687,999.6      3.0\n",
            "6 7,934,076.9     34.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRvzFop2zgYZ",
        "colab_type": "text"
      },
      "source": [
        "### **3.3. Output to JSON**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw6Bls2mjcka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # Convert dataframe to JSON, the required input for amCharts to generate the visualization.\n",
        "  result = result.to_json(orient = 'records')\n",
        "  result = json.loads(result, object_pairs_hook = OrderedDict)\n",
        "\n",
        "  # Make JSON format readable\n",
        "  result = json.dumps(result, indent = 4, sort_keys = False)\n",
        "\n",
        "  # Insert result to HTML environment\n",
        "  with open(out_path + k + '/' + str(y-1) + '_d_' + 'RHHHHINCOME' + '.js', 'w') as outfile:\n",
        "    outfile.write('var data =')\n",
        "    outfile.write(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvoHdYnNkXq6",
        "colab_type": "text"
      },
      "source": [
        "## **Section 6. Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nDHd4EAA_MN",
        "colab_type": "text"
      },
      "source": [
        "We now have four variables sorted by median for each state with population determining how many times we repeat the rows for each state:\n",
        "\n",
        "*   RHHINCOME: real household income\n",
        "*   ERHHINCOME: real household income per (equivalized) person)\n",
        "*   RHHINCOMERPP: real household income adjusted for state prices\n",
        "*   ERHHINCOMERPP: real household income per (equivalized) person) adjusted for state prices \n",
        "\n",
        "With the data prepared, we turn to amCharts to render the graph.\n",
        "\n",
        "**References:**\n",
        "\n",
        "[1] Sarah Flood, Miriam King, Renae Rodgers, Steven Ruggles and J. Robert Warren. Integrated Public Use Microdata Series, Current Population Survey: Version 6.0 [dataset]. Minneapolis, MN: IPUMS, 2018.\n",
        "https://doi.org/10.18128/D030.V6.0\n",
        "\n",
        "[2] https://cps.ipums.org/cps-action/variables/statefip#comparability_section \n",
        "\n",
        "[3] https://cps.ipums.org/cps/three_eighths.shtml).\n",
        "\n",
        "[4] Johnson, D., Smeeding T., and Boyle Torrey, B. \"Economic inequality through the prisms\n",
        "of income and consumption,\" *Monthly Labor Review* (Apr 2005), https://www.bls.gov/opub/mlr/2005/04/art2full.pdf.\n",
        "\n",
        "[5] Color generator: https://www.strangeplanet.fr/work/gradient-generator/index.php\n",
        "\n",
        "[6] AmChart documentation:  https://docs.amcharts.com/3/javascriptcharts/AmGraph\n",
        "\n",
        "[7] Jack Blundell's graph: https://jackblun.github.io/Globalinc/html/fig_1980.html\n"
      ]
    }
  ]
}